Using the NMT (Neural Machine Translation) architecture developed by Google.
Link: https://github.com/tensorflow/nmt

Encoder-decoder architecture:
    - Embedding:
        * A vocabulary of size 100.000 words is used
        * A word is represented as a vector in a 512-dimensional space
        * Unknown words are marked with a special symbol: <unk>
        * With training, related words (the representations) become closer
      
    - Encoder (a 2 layer dynamic bidirectional RNN, 512 nodes per layer)):
        * The entire input sentence is fed through an encoder
        * Encodes the entire sentence as a "thought vector"

    - Decoder (a 2 layer dynamic unidirectional RNN, 512 nodes per layer):
        * Decodes the "thought" sequentially into words.

Optimizations:
    - Adam optimizer
    - Attention mechanism - Scaled Luong
        * Shortcuts connections from input to output
        * Vast improvements on longer sentences
    - Beam search
        * Keep a running top (10) of best answers as output is generated
